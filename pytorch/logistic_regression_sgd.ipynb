{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "contemporary-purchase",
   "metadata": {},
   "source": [
    "# Logistic Regression in PyTorch\n",
    "\n",
    "Exploring how another one of the most basic machine learning models can be implemented using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-french",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "covered-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-chancellor",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "emotional-impression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbf5837d270>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-orbit",
   "metadata": {},
   "source": [
    "## Logistic Regression with the PyTorch Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-danish",
   "metadata": {},
   "source": [
    "Start by creating a dataset and dataloader for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nutritional-pledge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples = 400\n",
      "data[0] = (tensor([-2.]), tensor([0.]))\n",
      "mini_batch[0] = [tensor([[-2.0000],\n",
      "        [-1.9900],\n",
      "        [-1.9800],\n",
      "        [-1.9700],\n",
      "        [-1.9600]]), tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])]\n"
     ]
    }
   ],
   "source": [
    "class LinearlySeperableData(Dataset):\n",
    "    \n",
    "    def __init__(self, b: float, w: float, sigma: float = 0.1):\n",
    "        self.w = torch.tensor(w)\n",
    "        self.b = torch.tensor(b)\n",
    "        self.sigma = sigma\n",
    "        self.X = torch.arange(-2, 2, 0.01).view(-1, 1)\n",
    "\n",
    "        z = self.b + self.w * self.X\n",
    "        self.y = torch.where(z + self.sigma * torch.randn(self.X.size()) > 0, 1., 0.)\n",
    "        self.len = self.y.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx: float) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        return (self.X[idx], self.y[idx])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "data = LinearlySeperableData(b=0, w=1)\n",
    "print(f'n_samples = {len(data)}')\n",
    "print(f'data[0] = {data[0]}')\n",
    "\n",
    "data_loader = DataLoader(dataset=data, batch_size=5)\n",
    "data_batches = list(data_loader)\n",
    "print(f'mini_batch[0] = {data_batches[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-calculation",
   "metadata": {},
   "source": [
    "Now define the model. We will use a Binary Cross Entropy (BCE) loss function, which is equivalent to the negative of the log-likelhood function for a set of Bernouli trials - see [here](https://en.wikipedia.org/wiki/Cross_entropy#Relation_to_log-likelihood) for for information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "published-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionPyTorch(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int):\n",
    "        super(LogisticRegressionPyTorch, self).__init__()\n",
    "        # this is an alternative to torch.sigmoid(torch.nn.Linear())\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.criterion = torch.nn.BCELoss()  # this is a callable\n",
    "\n",
    "    def forward(self, X) -> torch.FloatTensor:\n",
    "        \"\"\"Compute a prediction.\"\"\"\n",
    "        return self.model(X)\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        data_loader: DataLoader,\n",
    "        n_epochs: int,\n",
    "        learning_rate: float\n",
    "    ) -> Sequence[float]:\n",
    "        \"\"\"Train the model over multiple epochs recording the loss for each.\"\"\"\n",
    "\n",
    "        def process_batch(X: torch.FloatTensor, y: torch.FloatTensor) -> float:\n",
    "            y_hat = self.forward(X)\n",
    "            loss = self.criterion(y_hat, y)\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            return loss.detach().numpy().tolist()\n",
    "\n",
    "        def process_epoch() -> float:\n",
    "            return [process_batch(X, y) for X, y in data_loader][-1]\n",
    "\n",
    "        optimiser = torch.optim.SGD(self.parameters(), lr=0.05)\n",
    "        training_run = [process_epoch() for epoch in range(n_epochs)]\n",
    "        return training_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-issue",
   "metadata": {},
   "source": [
    "We now training the model using `optim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "breathing-calculator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1400395780801773,\n",
       " 0.03897496312856674,\n",
       " 0.016650056466460228,\n",
       " 0.008784324862062931,\n",
       " 0.005244280211627483,\n",
       " 0.003394478466361761,\n",
       " 0.002325858222320676,\n",
       " 0.001662470051087439,\n",
       " 0.0012277166824787855,\n",
       " 0.0009306239662691951,\n",
       " 0.0007206659065559506,\n",
       " 0.0005679914029315114,\n",
       " 0.0004545174597296864,\n",
       " 0.00036841287510469556,\n",
       " 0.000302050553727895]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_clf = LogisticRegressionPyTorch(1)\n",
    "logistic_clf.fit(data_loader, n_epochs=15, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-geology",
   "metadata": {},
   "source": [
    "Take a look at estimated parameters. Note how uninportant (unconstrained) the slope parameter is for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dried-census",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.weight: [[4.0003342628479]]\n",
      "model.0.bias: [0.2258032113313675]\n"
     ]
    }
   ],
   "source": [
    "for k, v in logistic_clf.state_dict().items():\n",
    "    print(f'{k}: {v.numpy().tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-louisville",
   "metadata": {},
   "source": [
    "Testing the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "racial-norman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9850)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = LinearlySeperableData(b=0, w=1)\n",
    "\n",
    "y_hat = torch.where(logistic_clf.forward(test_data.X) > 0.5, 1., 0.)\n",
    "accuracy = torch.sum(torch.where(y_hat == test_data.y, 1., 0.)) / len(test_data)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-drain",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "It is not common to include the training routine as a model class method - e.g., `fit()`. Within the PyTorch community, it appears to be more commonly implemented as a seperate function. I'm adding SciKit-Learn conventions out of habit, more than anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-banana",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
